{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container {width:90% !important;}</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import time\n",
    "import warnings \n",
    "import requests\n",
    "import pandas as pd\n",
    "import pyperclip, re\n",
    "import json\n",
    "import math  # math.ceil 올림 / math.floor 내림 / math.trunc 버림\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm_notebook, tnrange\n",
    "from pandas import DataFrame\n",
    "from datetime import datetime, timedelta\n",
    "from IPython.core.display import display, HTML\n",
    "from konlpy.tag import Okt,Kkma,Hannanum,Komoran,Mecab   \n",
    "\n",
    "okt = Okt()\n",
    "kkma = Kkma()\n",
    "hannanum = Hannanum()\n",
    "komoran = Komoran\n",
    "mecab = Mecab # 윈도우에서 지원되지 않는다.\n",
    "\n",
    "# import datetime은 시간 (x)\n",
    "# datetime.date.today().timedelta(+10)\n",
    "\n",
    "display(HTML(\"<style>.container {width:90% !important;}</style>\"))\n",
    "\n",
    "pd.set_option(\"display.max_rows\", None , \"display.max_columns\", None)\n",
    "\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#i = 0\n",
    "#URL = []\n",
    "#for i in tnrange(0,1000):\n",
    "#while True:\n",
    "#    try: \n",
    "#        i +=1\n",
    "#        url = \"https://news.daum.net/breakingnews/society/affair?page=\"+str(i)+\"&regDate=20200906\"\n",
    "#        source = requests.get(url,verify=False).text\n",
    "#        soup = BeautifulSoup(source,\"html.parser\")\n",
    "#        soup2 = soup.select('.list_news2.list_allnews > li > a')\n",
    "#        if soup2 == []:\n",
    "#            break\n",
    "#        for url in soup2:\n",
    "#            URL.append(url['href'])\n",
    "#    except:\n",
    "#        break\n",
    "#URL\n",
    "\n",
    "#start_time = time.time()\n",
    "t = ['월','화','수','목','금','토','일']\n",
    "dt_index = pd.date_range(start='20170101', end='20171231')\n",
    "\n",
    "for date in tqdm_notebook(dt_index):\n",
    "\n",
    "    final = []\n",
    "\n",
    "    url_sample = 'http://news.daum.net'\n",
    "    url = 'https://news.daum.net/breakingnews/'\n",
    "    source = requests.get(url,verify=False).text\n",
    "    soup = BeautifulSoup(source,\"lxml\")\n",
    "    soup2 = soup.select('.tab_nav.tab_nav2 > li > a')\n",
    "    for url2 in tqdm_notebook(soup2):\n",
    "        source1 = requests.get(url_sample+url2['href'],verify=False).text\n",
    "        soup3 = BeautifulSoup(source1, \"lxml\")\n",
    "        soup4 = soup3.select('.tab_sub2 > li > a')\n",
    "        for url3 in tqdm_notebook(soup4[1:]):\n",
    "            for i in range(1,1000):\n",
    "                try:\n",
    "                    article_url = url_sample + url3['href'] + '?page=' + str(i) + '&regDate=' + date.strftime('%Y%m%d')\n",
    "                    print(article_url)\n",
    "                    source2 = requests.get(article_url,verify=False).text\n",
    "                    soup5 = BeautifulSoup(source2, \"lxml\")\n",
    "                    soup6 = soup5.select('.list_news2.list_allnews > li > div > strong > a')\n",
    "                    soup6_2 = soup5.select('.info_news')\n",
    "                    if soup6_2 == []:\n",
    "                        break\n",
    "                    for i in range(len(soup6)):\n",
    "                        URL = [soup6[i]['href']]\n",
    "                        Main_Category = [url2.text.replace('\\n','').strip()]\n",
    "                        Sub_Category = [url3.text.replace('\\n','').strip()]\n",
    "                        Date = [date.strftime('%Y%m%d')]\n",
    "                        year = [date.strftime('%Y')]\n",
    "                        month = [date.strftime('%m')]\n",
    "                        day = [date.strftime('%d')]\n",
    "                        wday = [t[date.weekday()]]\n",
    "                        title = [soup6[i].text]\n",
    "                        \n",
    "                        press = [soup6_2[i].text.replace(' ','').split('·')[0]]\n",
    "                        edittime = [soup6_2[i].text.replace(' ','').split('·')[1]]\n",
    "\n",
    "                        for item in zip(Date,year,month,day,wday,edittime,URL,Main_Category,Sub_Category,press,title):\n",
    "                            final.append(\n",
    "                                {\n",
    "                                    'Date' : item[0],\n",
    "                                    'year' : item[1],\n",
    "                                    'month': item[2],\n",
    "                                    'day'  : item[3],\n",
    "                                    'wday' : item[4],\n",
    "                                    'edittime': item[5],\n",
    "                                    'URL': item[6],\n",
    "                                    'Main_Category': item[7],\n",
    "                                    'Sub_Category' : item[8],\n",
    "                                    'press': item[9],\n",
    "                                    'title': item[10]\n",
    "                                }\n",
    "                            )\n",
    "\n",
    "                except:\n",
    "                    print(\"Connection refused by the server..\")\n",
    "                    print(\"Let me sleep for 10 seconds\")\n",
    "                    print(\"ZZzzzz...\")\n",
    "                    time.sleep(10)\n",
    "                    print(\"Was a nice sleep, now let me continue...\")\n",
    "                    continue\n",
    "\n",
    "    df = DataFrame(final)                   \n",
    "    df.to_csv(date.strftime('%Y%m%d') + '.csv',encoding='utf-8-sig',index=False) #utf-8-sig \n",
    "    df.to_csv(date.strftime('%Y%m%d') + '.csvd',encoding='utf-8-sig',index=False) #utf-8-sig\n",
    "    \n",
    "# print(\"--- elapsed time %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "# for문 시작할 때 빈 리스트\n",
    "    #    Date = []\n",
    "    #    year = []\n",
    "    #    month = []\n",
    "    #    day = []\n",
    "    #    wday = []\n",
    "    #    edittime = []\n",
    "    #    URL = []\n",
    "    #    Main_Category = []\n",
    "    #    Sub_Category = []\n",
    "    #    press = []\n",
    "    #    title = []\n",
    "\n",
    "        #    reporter_date_review_num = []      \n",
    "        #    content = []\n",
    "        #    for i in tqdm_notebook(URL):\n",
    "        #        source3 = requests.get(i,verify=False).text\n",
    "        #        soup7 = BeautifulSoup(source3,\"lxml\")\n",
    "        #        \n",
    "        #        reporter_date_review_num.append(soup7.select('.info_view')[0].text)\n",
    "        #        content.append(' '.join(soup7.select('.article_view > section')[0].text.split()))\n",
    "\n",
    "                           # source3 = requests.get(i['href'],verify=False).text\n",
    "                           # soup7 = BeautifulSoup(source3,\"lxml\")\n",
    "                           # reporter_date_review_num.append(soup7.select('.info_view')[0].text)\n",
    "\n",
    "                           # content.append(soup7.select('.article_view > section')[0].text.replace('\\n',' ').strip())\n",
    "                           # try:\n",
    "                           #     press.append(soup7.select('.link_cp > img')[0]['alt'])\n",
    "                           # except:\n",
    "                           #     press.append(soup7.select('.link_cp')[0].text.strip())\n",
    "\n",
    "#    df = pd.DataFrame(\n",
    "#        {'date':Date,\n",
    "#         'year':year,\n",
    "#         'month':month,\n",
    "#         'day':day,\n",
    "#         'wday':wday,\n",
    "#         'time':edittime,\n",
    "#        'url':URL,\n",
    "#        'main_Category':Main_Category,\n",
    "#        'sub_Category':Sub_Category,\n",
    "#        'press':press,\n",
    "#        'title':title\n",
    "    #        'reporter_date_review_num':reporter_date_review_num,\n",
    "    #        'content':content\n",
    "#        })\n",
    "\n",
    "#    df.to_csv(date.strftime('%Y%m%d') + '.csv',encoding='utf-8-sig',index=False) #utf-8-sig \n",
    "#    df.to_csv(date.strftime('%Y%m%d') + '.csvd',encoding='utf-8-sig',index=False) #utf-8-sig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('E:/[python]/5. [python] 자연어/data/daum/2020/20200101.csv')\n",
    "url = df['url']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def review_num(url):\n",
    "    resp = []\n",
    "    for i in tqdm_notebook(url):\n",
    "        url = 'https://comment.daum.net/apis/v1/ui/single/main/' + i.replace('https://v.daum.net/v/','@')\n",
    "        headers = {\n",
    "            'Authorization': 'Bearer eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJmb3J1bV9rZXkiOiJuZXdzIiwidXNlcl92aWV3Ijp7ImlkIjoxNjI1NTk5NiwiaWNvbiI6Imh0dHBzOi8vdDEuZGF1bWNkbi5uZXQvcHJvZmlsZS9SVlE4TnNOS1RTYzAiLCJwcm92aWRlcklkIjoiREFVTSIsImRpc3BsYXlOYW1lIjoi7JiB7KeEIn0sImdyYW50X3R5cGUiOiJhbGV4X2NyZWRlbnRpYWxzIiwic2NvcGUiOltdLCJleHAiOjE2MDQ0MjA2MjcsImF1dGhvcml0aWVzIjpbIlJPTEVfREFVTSIsIlJPTEVfSURFTlRJRklFRCIsIlJPTEVfVVNFUiJdLCJqdGkiOiI2YTg0ZGI1NC05YmFhLTQyYTYtYTJkYi0wMWM0NTc3NGM3N2EiLCJmb3J1bV9pZCI6LTk5LCJjbGllbnRfaWQiOiIyNkJYQXZLbnk1V0Y1WjA5bHI1azc3WTgifQ.RqsspGcWApMhPxHYmbPMil-59Q5xt3fOVvhoOiU3nk0',\n",
    "            'headers':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/86.0.4240.111 Safari/537.36'\n",
    "        }\n",
    "\n",
    "        resp += [json.loads(requests.get(url,headers=headers,verify=False).text.replace('post\\\":{\\\"','').replace('}}','}'))]\n",
    "    return(resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_num()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://comment.daum.net/apis/v1/ui/single/main/@20201102175449490'\n",
    "headers = {\n",
    "    'Authorization': 'Bearer eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJmb3J1bV9rZXkiOiJuZXdzIiwidXNlcl92aWV3Ijp7ImlkIjoxNjI1NTk5NiwiaWNvbiI6Imh0dHBzOi8vdDEuZGF1bWNkbi5uZXQvcHJvZmlsZS9SVlE4TnNOS1RTYzAiLCJwcm92aWRlcklkIjoiREFVTSIsImRpc3BsYXlOYW1lIjoi7JiB7KeEIn0sImdyYW50X3R5cGUiOiJhbGV4X2NyZWRlbnRpYWxzIiwic2NvcGUiOltdLCJleHAiOjE2MDQ0MjA2MjcsImF1dGhvcml0aWVzIjpbIlJPTEVfREFVTSIsIlJPTEVfSURFTlRJRklFRCIsIlJPTEVfVVNFUiJdLCJqdGkiOiI2YTg0ZGI1NC05YmFhLTQyYTYtYTJkYi0wMWM0NTc3NGM3N2EiLCJmb3J1bV9pZCI6LTk5LCJjbGllbnRfaWQiOiIyNkJYQXZLbnk1V0Y1WjA5bHI1azc3WTgifQ.RqsspGcWApMhPxHYmbPMil-59Q5xt3fOVvhoOiU3nk0',\n",
    "    'headers':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/86.0.4240.111 Safari/537.36'\n",
    "}\n",
    "resp = requests.get(url,headers=headers,verify=False)\n",
    "resp.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "resp = []\n",
    "for i in tqdm_notebook(url[0:10]):\n",
    "    url = 'https://comment.daum.net/apis/v1/ui/single/main/@' + i.replace('https://v.daum.net/v/','')\n",
    "    headers = {\n",
    "        'Authorization': 'Bearer eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJmb3J1bV9rZXkiOiJuZXdzIiwidXNlcl92aWV3Ijp7ImlkIjoxNjI1NTk5NiwiaWNvbiI6Imh0dHBzOi8vdDEuZGF1bWNkbi5uZXQvcHJvZmlsZS9SVlE4TnNOS1RTYzAiLCJwcm92aWRlcklkIjoiREFVTSIsImRpc3BsYXlOYW1lIjoi7JiB7KeEIn0sImdyYW50X3R5cGUiOiJhbGV4X2NyZWRlbnRpYWxzIiwic2NvcGUiOltdLCJleHAiOjE2MDQ0MjA2MjcsImF1dGhvcml0aWVzIjpbIlJPTEVfREFVTSIsIlJPTEVfSURFTlRJRklFRCIsIlJPTEVfVVNFUiJdLCJqdGkiOiI2YTg0ZGI1NC05YmFhLTQyYTYtYTJkYi0wMWM0NTc3NGM3N2EiLCJmb3J1bV9pZCI6LTk5LCJjbGllbnRfaWQiOiIyNkJYQXZLbnk1V0Y1WjA5bHI1azc3WTgifQ.RqsspGcWApMhPxHYmbPMil-59Q5xt3fOVvhoOiU3nk0',\n",
    "        'headers':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/86.0.4240.111 Safari/537.36'\n",
    "    }\n",
    "    resp += [json.loads(requests.get(url,headers=headers,verify=False).text.replace('post\\\":{\\\"','').replace('}}','}'))]\n",
    "\n",
    "resp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from multiprocessing import Pool, Manager, freeze_support, RLock\n",
    "from tqdm import tqdm, trange, tqdm_notebook\n",
    "import workers\n",
    "\n",
    "import requests\n",
    "import json\n",
    "\n",
    "df = pd.read_csv('E:/[python]/5. [python] 자연어/data/daum/2020/20200101.csv')\n",
    "url = df['url']\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    freeze_support()\n",
    "    start_time = time.time()\n",
    "    pool = Pool(processes=4)#4개의 프로세스 동시에 작동\n",
    "    pool.map(workers.review_num,url) #title_to_list라는 함수에 1 ~ end까지 10씩늘려가며 인자로 적용\n",
    "    print(\"실행 시간 : %s초\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "url = 'https://comment.daum.net/apis/v1/ui/single/main/@20201102175449490'\n",
    "headers = {\n",
    "    'Authorization': 'Bearer eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJmb3J1bV9rZXkiOiJuZXdzIiwidXNlcl92aWV3Ijp7ImlkIjoxNjI1NTk5NiwiaWNvbiI6Imh0dHBzOi8vdDEuZGF1bWNkbi5uZXQvcHJvZmlsZS9SVlE4TnNOS1RTYzAiLCJwcm92aWRlcklkIjoiREFVTSIsImRpc3BsYXlOYW1lIjoi7JiB7KeEIn0sImdyYW50X3R5cGUiOiJhbGV4X2NyZWRlbnRpYWxzIiwic2NvcGUiOltdLCJleHAiOjE2MDQ0MDUxMjQsImF1dGhvcml0aWVzIjpbIlJPTEVfREFVTSIsIlJPTEVfSURFTlRJRklFRCIsIlJPTEVfVVNFUiJdLCJqdGkiOiJmMTlhMGFlMS1lOWVhLTQwMjEtOGM0Yi1iMmIzYzE2NzdmMDQiLCJmb3J1bV9pZCI6LTk5LCJjbGllbnRfaWQiOiIyNkJYQXZLbnk1V0Y1WjA5bHI1azc3WTgifQ.q6ddFa15X0s2O6rZkDL65jX9WKaEF3dZj_FRO5dhki8',\n",
    "    'headers':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/86.0.4240.111 Safari/537.36'\n",
    "}\n",
    "source = requests.get(url,headers=headers,verify=False).text\n",
    "idid = json.loads(source)['post']['id']\n",
    "commentCount = json.loads(source)['post']['commentCount']\n",
    "childCount = json.loads(source)['post']['childCount']\n",
    "review_num = commentCount - childCount\n",
    "\n",
    "offset = 0\n",
    "A = []\n",
    "for i in tnrange(math.ceil(review_num/100)):\n",
    "    url = 'https://comment.daum.net/apis/v1/posts/' + str(idid) +\\\n",
    "            '/comments?parentId=0&offset='+ str(offset) +\\\n",
    "             '&limit=100&sort=RECOMMEND&isInitial=false&hasNext=true'\n",
    "     \n",
    "    source = requests.get(url,headers=headers,verify=False)\n",
    "    A += json.loads(source.text.replace('}}','}').replace('\\\"user\\\":{\\\"id','\\\"id2').replace('\\\\n\\\\n',' ').replace('\\\\n',' ').replace('\\\\uD',''))\n",
    "    offset += 100\n",
    "\n",
    "\n",
    "#DataFrame(json.loads(source.text.replace('}}','}').replace('\\\"user\\\":{','').replace('\\\\n',' ')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "userid = DataFrame(A)['userId']\n",
    "\n",
    "B = []\n",
    "for i in tnrange(10):\n",
    "    url = 'https://comment.daum.net/apis/v1/comments/by/' + str(userid[i]) + '/in/-99/count'\n",
    "    review_num = requests.get(url,headers=headers,verify=False).text\n",
    "    print(review_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "userid = DataFrame(A)['userId']\n",
    "\n",
    "B = []\n",
    "for i in tnrange(10):\n",
    "    url = 'https://comment.daum.net/apis/v1/comments/by/' + str(userid[i]) + '/in/-99/count'\n",
    "    review_num = requests.get(url,headers=headers,verify=False).text\n",
    "\n",
    "    for j in tnrange(math.ceil(int(review_num)/100)):\n",
    "        url = 'https://comment.daum.net/apis/v1/users/'+ str(userid[i]) + '/comments?offset='+ str(j*100) + '&limit=100&sort=LATEST&hasNext=true&forumId=-99'\n",
    "        source = requests.get(url,headers=headers,verify=False)\n",
    "        B += json.loads(source.text.\\\n",
    "                        replace('}}','}').\\\n",
    "                        replace('}}','}}]}').\\\n",
    "                        replace('\\\"post\\\":{\\\"id', '\\\"postId').\\\n",
    "                        replace('},\\\"user', ',\\\"user').\\\n",
    "                        replace('},\\\"parent\\\":', ',\\\"parent\\\":[').\\\n",
    "                        replace('\\\\n\\\\n',' ').replace('\\\\n',' ').\\\n",
    "                        replace('\\\\uD',''))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = DataFrame(B)\n",
    "\n",
    "df.to_csv('test.csv',encoding = 'utf-8-sig')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
